---
title: ""
author: ""
date: "2/17/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Reading in Libraries

```{r}
library(ISLR)
library(MASS)
library(caret)
library(leaps)
library(glmnet)
library(pls)
```

## 6.8 a) Generating Random X and Noise Vectors

```{r}
set.seed(814)
x = rnorm(100)
noise = rnorm(100)
```

## 6.8 b) Generating Response Vector Y

```{r}
y = (0.1) + (0.2)*x + (0.3)*x^2 + (0.4)*x^3 + noise
```

## 6.8 c) Predictor Selection using Regsubsets

```{r}
df <- data.frame(x,y)
colnames(df) <- c("x","y")
reg <- regsubsets(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6)
                  + I(x^7) + I(x^8) + I(x^9) + I(x^10), data = df, nvmax = 10)
summary(reg)

summary(reg)$rsq
summary(reg)$bic
summary(reg)$cp

plot(summary(reg)$rsq, xlab = "Number of Variables", ylab = "R-Squared")
plot(summary(reg)$bic, xlab = "Number of Variables", ylab = "BIC")
plot(summary(reg)$cp, xlab = "Number of Variables", ylab = "Cp")
```

If we just look to the R-squared, we should use the maximum number of predictors, that being ten In terms of minimizing the BIC, our results state that we should use two. However, when using the Cp statistic, four is the optimal number of variables.

## 6.8 d) Regsubsets with Forward and Backward Selection

```{r}
reg <- regsubsets(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6)
                  + I(x^7) + I(x^8) + I(x^9) + I(x^10), data = df,
                  method = "forward", nvmax = 10)
summary(reg)

summary(reg)$rsq
summary(reg)$bic
summary(reg)$cp

plot(summary(reg)$rsq, xlab = "Number of Variables", ylab = "R-Squared")
plot(summary(reg)$bic, xlab = "Number of Variables", ylab = "BIC")
plot(summary(reg)$cp, xlab = "Number of Variables", ylab = "Cp")

reg <- regsubsets(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6)
                  + I(x^7) + I(x^8) + I(x^9) + I(x^10), data = df,
                  method = "backward", nvmax = 10)
summary(reg)

summary(reg)$rsq
summary(reg)$bic
summary(reg)$cp

plot(summary(reg)$rsq, xlab = "Number of Variables", ylab = "R-Squared")
plot(summary(reg)$bic, xlab = "Number of Variables", ylab = "BIC")
plot(summary(reg)$cp, xlab = "Number of Variables", ylab = "Cp")
```

Using forward selection, minimizing the BIC statistic leads us to same result, using two variables. When we emphasize Cp, we use three variables, which is the true number of predictors in our data generating process. According to backward selection, we ought to use two and four variables when minimizing the BIC and Cp statistics respectively.

## 6.8 e) Fitting a Lasso Model

```{r}
x.poly <- as.matrix(data.frame(x, I(x^2), I(x^3), I(x^4), I(x^5), I(x^6),
                               I(x^7), I(x^8), I(x^9), I(x^10)))
train <- sample(100, 70)

cv.out <- cv.glmnet(x.poly[train,], y[train], alpha = 1)
plot(cv.out)
best.lam <- cv.out$lambda.min

grid <- 10^seq(10, -2, length = 100)
lasso.mod <- glmnet(x.poly[train,], y[train], alpha = 1, lambda = grid)
plot(lasso.mod)

out <- glmnet(x.poly, y, alpha = 1, lambda = grid)
lasso.coef <- predict(out, type = "coefficients", s = best.lam)
lasso.coef
```

The coefficients generated by the Lasso model provide a generally poor approximation for the true data generating process. Most importantly, the model includes a coefficient for $x^4$ when none was included in our model, which has likely "stolen" some magnitude from our $x^2$ coefficient. The coefficient for $x$ is inflated by a factor of 3, while that for $x^3$ is slightly less than its actual value. 

## 6.8 f) Regsubsets and Lasso with New Response Vector

```{r}
y <- (0.1) + (0.8)*x^7 + noise

df <- data.frame(x,y)
colnames(df) <- c("x","y")
reg <- regsubsets(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6)
                  + I(x^7) + I(x^8) + I(x^9) + I(x^10), data = df, nvmax = 10)
summary(reg)

summary(reg)$rsq
summary(reg)$bic
summary(reg)$cp

plot(summary(reg)$rsq, xlab = "Number of Variables", ylab = "R-Squared")
plot(summary(reg)$bic, xlab = "Number of Variables", ylab = "BIC")
plot(summary(reg)$cp, xlab = "Number of Variables", ylab = "Cp")

x.poly <- as.matrix(data.frame(x, I(x^2), I(x^3), I(x^4), I(x^5), I(x^6),
                               I(x^7), I(x^8), I(x^9), I(x^10)))
train <- sample(100, 70)

cv.out <- cv.glmnet(x.poly[train,], y[train], alpha = 1)
plot(cv.out)
best.lam <- cv.out$lambda.min

grid <- 10^seq(10,-2,length=100)
lasso.mod <- glmnet(x.poly[train,], y[train], alpha = 1, lambda = grid)
plot(lasso.mod)

out <- glmnet(x.poly, y, alpha = 1, lambda = grid)
lasso.coef <- predict(out, type = "coefficients", s = best.lam)
lasso.coef
```

According to all three statistics, R-squared, BIC, and Cp, the optimal number of variables is one, which is correct from the model we designed. The Lasso model correctly states that only one predictor should be used, that being $x^7$, with a coefficient of 0.77, which is very close to the actual value. The intercept is much further from the actual value, so it is likely that the intercept estimate suffers from higher variance. It ultimately appears that the Lasso model performs better in predicting the true model when the data generating process involves fewer predictors.